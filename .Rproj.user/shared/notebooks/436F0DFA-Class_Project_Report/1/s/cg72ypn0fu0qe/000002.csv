"0","Tokenization_fun <- function(df){"
"0","  df$text = gsub(""(f|ht)(tp)(s?)(://)(.*)[.|/](.*)"", "" "", df$text)"
"0","  "
"0","  #removing link"
"0","  df$text = gsub(""(f|ht)(tp)(s?)(://)(.*)[.|/](.*)"", "" "", df$text)"
"0",""
"0","  # removing hashtags"
"0","  df$text = gsub(""#\\w+"", "" "", df$text)"
"0",""
"0","  # removing @people"
"0","  df$text = gsub(""@\\w+"", "" "", df$text)"
"0",""
"0","  #removing punctuations"
"0","  df$text = gsub(""[[:punct:]]"", "" "", df$text)"
"0",""
"0","  #removing numbers"
"0","  df$text = gsub(""[[:digit:]]"", "" "", df$text)"
"0",""
"0","  #removing emojis"
"0","  df$text <- str_replace_all(df$text,""[^[:graph:]]"","" "")"
"0","  df$text <- str_replace_all(df$text,'https',"" "")"
"0","  df$text <- str_replace_all(df$text,'amp',"" "")"
"0",""
"0","  #removing spaces"
"0","  df$text = gsub(""[ \t]{2,}"", "" "", df$text)"
"0","  df$text = gsub(""^\\s+|\\s+$"", """", df$text)"
"0","  "
"0","  return(df)"
"0","}"
"0",""
"0","Taxi.tweets <- Tokenization_fun(Taxi.tweets)"
"0","Uber.tweets <- Tokenization_fun(Uber.tweets)"
"0","Lyft.tweets <- Tokenization_fun(Lyft.tweets)"
"0","RSS.tweets <- rbind(Taxi.tweets, Uber.tweets, Lyft.tweets)"
"0",""
"0",""
"0","Amtrak.tweets <- Tokenization_fun(Amtrak.tweets)"
"0","metro.tweets <- Tokenization_fun(metro.tweets)"
"0","metrobus.tweets <- Tokenization_fun(metrobus.tweets)"
"0","PT.tweets <- rbind(Amtrak.tweets, metro.tweets, metrobus.tweets)"
